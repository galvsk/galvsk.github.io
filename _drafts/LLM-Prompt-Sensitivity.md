---
layout: post
title: "Your Title Here"
description: "A brief description of your post that will appear in previews and SEO"
categories: [optional, categories, here]
---

This blogpost summarises my research project for the BlueDot Impact AI Alignment course [[1]](#references). I'm greatly appreciative of the learning and support throughout the 12-week course, and highly recommend any technical people curious about the state of AI safety and alignment research to consider applying! The project phase gives enrollees an opportunity to conduct independent research on a topic they found interesting in the learning phase. I chose to focus on robustness, as it's a topic I'm familiar with as a healthcare ML researcher, with an emphasis on determining how brittle frontier LLMs are when relatively simple formatting changes are applied to input prompts. All of the code for generating the benchmark data, running the variousevaluations, and statistical analysis can be found in the following github repo [[2]](#references). Despite my critical investigations, I would like to acknowledge Claude 3.5 Sonnet, which I used in writing portions of the codebase and this report.

## Introduction

In this post, I'll be discussing... [outline your main topic and why it matters]

Key points that will be covered:
- First major point or theme
- Second major point or theme
- What readers will learn or take away


## References

[1]: AI Safety Fundamentals. "AI Alignment Course" <https://aisafetyfundamentals.com/alignment/>
[2]: MMLU Evaluation Code. <https://github.com/galvsk/mmlu-eval>
