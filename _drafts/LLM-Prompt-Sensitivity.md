---
layout: post
title: "Unmasking Prompt Sensitivity: A Deep Dive into Language Model Performance"
description: "Evaluating 0-shot prompt sensitivity of Claude 3.5 Sonnet and DeepSeek-v3 on MMLU"
categories: [optional, categories, here]
---

This blog post summarises my research project for the BlueDot Impact AI Alignment course [[1]](#references). I'm greatly appreciative of the learning and support (including the compute credits to run the following evals) throughout the 12-week course, and highly recommend any technical people curious about the state of AI safety and alignment research to consider applying! The project phase gives enrollees an opportunity to conduct independent research on a topic they found interesting in the learning phase. I chose to focus on robustness, as it's a topic I'm familiar with as a healthcare ML researcher, with an emphasis on determining how brittle frontier LLMs are when relatively simple formatting changes are applied to input prompts. All of the code for generating the benchmark data, running the various evaluations, and statistical analysis, can be found in the following github repo [[2]](#references). Despite my critical investigations, I would like to acknowledge Claude 3.5 Sonnet, which I collaborated with in writing portions of the codebase and this post.

## Abstract

This study investigates the robustness of Claude 3.5 Sonnet and DeepSeek-v3 to prompt perturbations on the Massive Multitask Language Understanding (MMLU) benchmark in a zero-shot setting. The research first established baseline performance across both training and test sets to assess potential train-test generalisation gaps, addressing concerns about test set contamination in large language models (LLMs). 

The experimental methodology systematically evaluated model performance under various prompt modifications: (i) permuting multiple-choice answer orderings, (ii) uniform uppercase transformation, (iii) random casing transformations, and (iv) duplication of incorrect answer choices. Additionally, the study examined model behaviour when presented with incorrect answers generated by the models themselves on a subset of the test data, probing both potential self-consistency biases and the models' ability to reason about previously generated incorrect responses.

Results reveal distinct robustness profiles between the two models. DeepSeek-v3 demonstrated remarkable stability across syntactic prompt modifications but showed significant bias towards its self-generated incorrect answers. In contrast, Claude 3.5 Sonnet's performance exhibited statistically significant sensitivity to all syntactic perturbations, yet displayed superior reasoning capabilities when evaluating LLM-generated incorrect answers.

Results reveal distinct robustness profiles between the two models. DeepSeek-v3 demonstrated remarkable stability across syntactic prompt modifications but showed significant bias towards its self-generated incorrect answers. In contrast, Claude 3.5 Sonnet's performance exhibited statistically significant sensitivity to all syntactic perturbations, yet displayed superior reasoning capabilities when evaluating LLM-generated incorrect answers. This variability raises important questions about the relationship between training methodology and emergent model behaviours, suggesting that seemingly minor implementation differences may lead to substantial variations in model robustness and reasoning patterns. These divergences suggests fundamental differences in their underlying training approaches, providing empirical evidence that DeepSeek-v3's capabilities likely emerge from independent architectural and training innovations rather than direct replication of existing models. 


## Introduction

In this post, I'll be discussing... [outline your main topic and why it matters]

Key points that will be covered:
- First major point or theme
- Second major point or theme
- What readers will learn or take away


## References

1. AI Safety Fundamentals. "AI Alignment Course" <https://aisafetyfundamentals.com/alignment/>

2. MMLU evaluation codebase. <https://github.com/galvsk/mmlu-eval>
