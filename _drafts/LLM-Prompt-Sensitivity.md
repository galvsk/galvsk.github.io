---
layout: single
title: "Unmasking Prompt Sensitivity in LLMs"
description: "Evaluating the zero-shot prompt sensitivity of Claude 3.5 Sonnet and DeepSeek-v3 on MMLU"
---

This blog post summarises my research project for the BlueDot Impact's AI Alignment course [[1]](#references). I'm greatly appreciative of the learning and support (including the compute credits to run the following evals) throughout the 12-week course, and highly recommend any technical people curious about the state of AI safety and alignment research to consider applying! The project phase gives enrollees an opportunity to conduct independent research on a topic they found interesting in the learning phase. I chose to focus on robustness, as it's a topic I'm familiar with as a healthcare ML researcher, with an emphasis on determining how brittle frontier LLMs are when relatively simple changes are applied to input prompts. All of the code for generating the benchmark data, running the various evaluations, and statistical analysis, can be found in the following github repo [[2]](#references). Despite my critical investigations, I would like to acknowledge Claude 3.5 Sonnet, which I collaborated with in writing portions of the codebase and this post.

## Abstract

This study investigates the robustness of Claude 3.5 Sonnet and DeepSeek-v3 to prompt perturbations on the Massive Multitask Language Understanding (MMLU) benchmark [[3]](#references) in a zero-shot setting. The research first established baseline performance across both training and test sets to assess potential train-test generalisation gaps, addressing concerns about test set contamination in large language models (LLMs). 

The experimental methodology systematically evaluated model performance under various prompt modifications: (i) permuting multiple-choice answer orderings, (ii) uniform uppercase transformation, (iii) random casing transformations, and (iv) duplication of incorrect answer choices. Additionally, the study examined model behaviour when presented with incorrect answers generated by the models themselves on a subset of the test data, probing both potential self-consistency biases and the models' ability to reason about previously generated incorrect responses.

Results reveal distinct robustness profiles between the two models. DeepSeek-v3 demonstrated stability across syntactic prompt modifications but showed significant bias towards its self-generated incorrect answers. In contrast, Claude 3.5 Sonnet's performance exhibited statistically significant sensitivity to all syntactic perturbations, yet displayed superior reasoning capabilities when evaluating LLM-generated incorrect answers.

This variability raises important questions about the relationship between training methodology and emergent model behaviours, suggesting implementation differences may lead to substantial variations in model robustness and reasoning patterns. These fundamental differences also provide empirical evidence that DeepSeek-v3's capabilities likely emerge from independent architectural and training innovations rather than direct replication of existing models. 

## Introduction

In 2020, GPT-3 demonstrated high school level language understanding [[4]](#references). By 2024, iAsk Pro exceeded expert performance across all domains in the MMLU-Pro benchmark [[5]](#references). This pace of progress is unprecedented - before deep learning, natural language processing (NLP) benchmarks typically lasted a decade [[6-8]](#references). When deep learning emerged around 2014 [[9, 10]](#references), this shortened to 2-3 years. Now, even carefully designed benchmarks like SuperGLUE [[11]](#references) are often saturated within months, but they are not without flaws. While they remain our best measure for tracking general intelligence capabilities, we must carefully examine how sensitive models are to spurious signals in their training distributions. Furthermore, despite their utility and widespread adoption, reducing general capabilities to a single number is inherently imprecise [[12]](#references). They also serve as crucial marketing tools for frontier labs, with current market competition raising concerns about direct optimisation for test set performance [[13]](#references).

While recent focus has shifted toward evaluating more complex and general reasoning capabilities [[14]](#references), including most notably the recent breakthrough for OpenAI's o3 model on the ARC-AGI-benchmark [[15]](#references), I believe fundamental questions about robustness remain both important, and unresolved. For instance, extensive research has been reported on how LLM performance drops when simple formatting changes are applied to the input prompt [[16-18]](#references), and even larger drops for irrelevant content changes [[19]](#references). This brittleness to input phrasing is particularly concerning for safety-critical applications, as these models are increasingly considered in domains like healthcare, legal systems, and national security. I believe frontier labs are also taking this brittleness seriously. The recent Llama 3 technical report [[20]](#references), which was the most cited DL paper of 2024 (despite being published in July), had four figures explicitly evaluating prompt sensitivity on MMLU.

As frontier labs exhaust high-quality internet data, and human preference labelling limits post-training scaling, they're increasingly turning to LLM-generated content for breakthroughs [[21, 22]](#references). Even benchmarks are following this trend - MMLU-Pro utilised GPT-4-Turbo to expand answer choices from four to ten [[23]](#references). This creates multiple feedback loops: newer models are trained on content from older models, potentially on other labs' model outputs, and evaluated on benchmarks containing LLM-generated content. Research suggests this could be problematic, as LLMs show particular biases toward their own outputs [[24]](#references), raising questions about how this recursive dynamic might amplify biases as well as capabilities.

This project investigates three key questions about benchmark reliability and model behaviour. First, while test set performance is widely reported, training set performance is rarely evaluated - making it impossible to assess training-to-test generalisation. By evaluating both, this project provides novel insight into the generalisation gap on MMLU. Second, while prompt sensitivity was well documented in earlier models, how robust are current frontier models to simple formatting changes? Have frontier labs solved this seemingly trivial issue? And third, how do these models handle incorrect content generated by themselves and other LLMs - this helps us understand whether there's a general bias towards all LLM-generated content, or a specific bias towards their own outputs. Understanding these questions becomes increasingly important as LLM-generated content plays a larger role in both training and evaluation. 

## Experiments and findings

All prompts were submitted in a zero-shot fashion. This was mainly done due to simplicity, but coincidentally recent research suggest zero-shot outperforms few-shot prompting [[25]](#references). Few-shot prompting is likely a relic required to elicit latent knowledge in significantly less sophisticated language models. Experiments were conducted on Claude 3.5 Sonnet and DeepSeek-v3. Both models were accessed through their respective APIs, which do not expose logprobs or logits, preventing any calibration analysis of model outputs. All statistical measurements were calculated using bootstrap resampling with 10,000 samples (sampling with replacement), yielding 95% confidence intervals (CIs). Non-overlapping CIs provide a conservative test of statistical significance (p < 0.05), equivalent to a two-sample bootstrap test with a reduced Type I error rate.

### Train to test generalisation

MMLU consists of 99,842 training and 14,042 test questions. Category labels for questions exist only for the test set, and consist of 57 different subjects. I got Claude 3.5 Sonnet to simplify these to eight coarser labels (the translation table can be found here [[26]](#references)), with final subject category labels being : STEM, Law & Ethics, Social Sciences, Medicine & Health, Humanities, Business & Professional, Global Studies, and Other.


## References

1. AI Safety Fundamentals. <https://aisafetyfundamentals.com/alignment/>

2. MMLU evaluation codebase. <https://github.com/galvsk/mmlu-eval>

3. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt, J. (2021). **Measuring Massive Multitask Language Understanding**. arXiv:2009.03300.

4. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., ... Amodei, D. (2020). **Language Models are Few-Shot Learners.** arXiv preprint arXiv:2005.14165.

5. iAsk Pro's MMLU-Pro Benchmark Results. <https://iask.ai/mmlu-pro>

6. Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). **Building a large annotated corpus of English: The Penn Treebank**. Computational Linguistics, 19(2).

7. Tjong Kim Sang, E. F., & De Meulder, F. (2003). **Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition**. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003.

8. Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002). **BLEU: a method for automatic evaluation of machine translation**. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL).

9. Sutskever, I., Vinyals, O., & Le, Q. V. (2014). **Sequence to sequence learning with neural networks**. Advances in neural information processing systems, 27.

10. Bahdanau, D., Cho, K., & Bengio, Y. (2014). **Neural machine translation by jointly learning to align and translate**. arXiv preprint arXiv:1409.0473.

11. Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. (2019). **SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems**. Advances in Neural Information Processing Systems, 32.

12. Zheng, L., Chiang, W. L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., & Li, H. (2023). **Judging LLM-as-a-judge with MT-Bench and Chatbot Arena**. arXiv preprint arXiv:2306.05685.

13. Oren, Y., Meister, N., Chatterji, N., Ladhak, F., & Hashimoto, T. B. (2023). **Proving test set contamination in black box language models**. arXiv preprint arXiv:2310.17623.

14. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., & Cobbe, K. (2023). **Let's Verify Step by Step**. arXiv:2305.20050.

15. OpenAI o3 ARC-AGI results. <https://arcprize.org/blog/oai-o3-pub-breakthrough>

16. Zheng, C., Zhou, H., Meng, F., Zhou, J., & Huang, M. (2024). **Large Language Models Are Not Robust Multiple Choice Selectors**. arXiv:2309.03882.

17. Sclar, M., Choi, Y., Tsvetkov, Y., & Suhr, A. (2024). **Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting**. arXiv:2310.11324.

18. Alzahrani, N., Alyahya, H. A., Alnumay, Y., Alrashed, S., Alsubaie, S., Almushaykeh, Y., et al. (2024). **When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards**. arXiv:2402.01781.

19. Mirzadeh, I., Alizadeh, K., Shahrokhi, H., Tuzel, O., Bengio, S., & Farajtabar, M. (2024). **GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models**. arXiv:2410.05229.

20. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., et al. (2024). **The Llama 3 Herd of Models**. arXiv:2407.21783.

21. Scheurer, J., Campos, J. A., Chan, J. S., Chen, A., Cho, K., & Perez, E. (2022). **Training Language Models with Language Feedback**. arXiv:2204.14146.

22. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., et al. (2022). **Constitutional AI: Harmlessness from AI Feedback**. arXiv:2212.08073.

23. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., et al. (2024). **MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark**. arXiv:2406.01574.

24. Panickssery, A., Bowman, S. R., & Feng, S. (2024). **LLM Evaluators Recognize and Favor Their Own Generations**. arXiv:2404.13076.

25. DeepSeek-r1 technical report (2025). <https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf>

26. 
