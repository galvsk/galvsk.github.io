---
layout: single
title: "Benchmarks and Brittleness: Evaluating Frontier LLM Robustness"
description: "Evaluating the zero-shot prompt sensitivity of Claude 3.5 Sonnet and DeepSeek-v3 on MMLU"
---

This blog post summarises my research project for BlueDot Impact's AI Alignment course [[1]](#references). I'm greatly appreciative of the learning and support (including the compute credits to run the following evals) throughout the 12-week course, and highly recommend any technical people curious about the state of AI safety and alignment research to consider applying! The project phase gives enrollees an opportunity to conduct independent research on a topic they found interesting in the learning phase. I chose to focus on robustness, as it's a topic I'm familiar with as a healthcare ML researcher, with an emphasis on determining how brittle frontier LLMs are when relatively simple changes are applied to input prompts. All of the code for generating the benchmark data, running the various evaluations, and statistical analysis, can be found in the following github repo [[2]](#references). Despite my critical investigations, I would like to acknowledge Claude 3.5 Sonnet, which I collaborated with in writing the codebase and this post.

## Abstract

This study investigates the robustness of Claude 3.5 Sonnet and DeepSeek-v3 to prompt perturbations on the Massive Multitask Language Understanding (MMLU) benchmark [[3]](#references) in a zero-shot setting. The research first established baseline performance across both training and test sets to assess potential train-test generalisation gaps, addressing concerns about test set contamination in large language model (LLM) model training. 

The experimental methodology systematically evaluated model performance under various prompt modifications: (i) permuting multiple-choice answer orderings, (ii) uniform uppercase transformation, (iii) random casing transformations, and (iv) duplication of incorrect answer choices. Additionally, the study examined model behaviour when presented with incorrect answers generated by the models themselves on a subset of the test data, probing both potential self-consistency biases and the models' ability to reason about previously generated incorrect responses.

Results reveal distinct robustness profiles between the two models. DeepSeek-v3 demonstrated stability across syntactic prompt modifications but showed significant bias towards its self-generated incorrect answers. In contrast, Claude 3.5 Sonnet's performance exhibited statistically significant sensitivity to all syntactic perturbations, yet displayed superior reasoning capabilities when evaluating LLM-generated incorrect answers.

This variability raises important questions about the relationship between training methodology and emergent model behaviours, suggesting implementation differences may lead to substantial variations in model robustness and reasoning patterns. These fundamental differences also provide empirical evidence that DeepSeek-v3's capabilities likely emerge from independent architectural and training innovations rather than direct replication of existing models. 

## Introduction

In 2020, GPT-3 demonstrated high school level language understanding [[4]](#references). By 2025, DeepSeek-r1 exceeded expert performance across multiple domains in the MMLU-Pro benchmark [[5]](#references). This pace of progress is unprecedented - before deep learning, natural language processing (NLP) benchmarks typically lasted a decade [[6-8]](#references). When DL started to be successfully applied to NLP around 2014 [[9, 10]](#references), this shortened to 2-3 years. Now, even carefully designed benchmarks like SuperGLUE [[11]](#references) are often saturated within months, but they are not without flaws. While they remain our best measure for tracking general intelligence capabilities, we must carefully examine how sensitive models are to spurious signals in their training distributions. Furthermore, despite their utility and widespread adoption, reducing general capabilities to a single number is inherently imprecise [[12]](#references). They also serve as crucial marketing tools for frontier labs, with current market competition raising concerns about direct optimisation for test set performance [[13]](#references).

While recent focus has shifted toward evaluating more complex and general reasoning capabilities [[14]](#references), including most notably the recent breakthrough for OpenAI's o3 model on the ARC-AGI-benchmark [[15]](#references), I believe fundamental questions about robustness remain both important, and unresolved. For instance, extensive research has been reported on how LLM performance drops when simple formatting changes are applied to the input prompt [[16-18]](#references), and even larger drops for irrelevant content changes [[19]](#references). This brittleness to input phrasing is particularly concerning for safety-critical applications, as these models are increasingly considered in domains like healthcare, legal systems, and national security. I believe frontier labs are also taking this brittleness seriously. The recent Llama 3 technical report [[20]](#references), which was the most cited DL paper of 2024 (despite being published in July), had four figures explicitly evaluating prompt sensitivity on MMLU.

As frontier labs exhaust high-quality internet data, and human preference labelling limits post-training scaling, they're increasingly turning to LLM-generated content for breakthroughs [[21, 22]](#references). Even benchmarks are following this trend - MMLU-Pro utilised GPT-4-Turbo to expand answer choices from four to ten [[23]](#references). This creates multiple feedback loops: newer models are trained on content from older models, potentially on other labs' model outputs, and evaluated on benchmarks containing LLM-generated content. Research suggests this could be problematic, as LLMs show particular biases toward their own outputs [[24]](#references), raising questions about how this recursive dynamic might amplify biases as well as capabilities.

This project investigates three key questions about benchmark reliability and model behaviour. First, while test set performance is widely reported, training set performance is rarely evaluated - making it impossible to assess training-to-test generalisation. By evaluating both, this project provides novel insight into the generalisation gap on MMLU. Second, while prompt sensitivity was well documented in earlier models, how robust are current frontier models to simple formatting changes? Have frontier labs solved this seemingly trivial issue? And third, how do these models handle incorrect content generated by themselves and other LLMs - this helps us understand whether there's a general bias towards all LLM-generated content, or a specific bias towards their own outputs. Understanding these questions becomes increasingly important as LLM-generated content plays a larger role in both training and evaluation. 

## Experimental setup and findings

All prompts were submitted in a zero-shot fashion. This was mainly done due to simplicity, but coincidentally recent research suggest zero-shot outperforms few-shot prompting [[25]](#references). Few-shot prompting is likely a relic required to elicit latent knowledge in significantly less sophisticated language models. Experiments were conducted on Claude 3.5 Sonnet and DeepSeek-v3. Both models were accessed through their respective APIs, which do not expose logprobs or logits, preventing any calibration analysis of model outputs. All statistical measurements were calculated using bootstrap resampling with 10,000 samples (sampling with replacement), yielding 95% confidence intervals (CIs). Non-overlapping CIs provide a conservative test of statistical significance (p < 0.05), equivalent to a two-sample bootstrap test with a reduced Type I error rate.

### (i) Train to test generalisation

MMLU consists of 99,842 training and 14,042 test questions. Category labels for questions exist only for the test set, and consist of 57 different subjects. I got Claude 3.5 Sonnet to simplify these to eight coarser labels (the translation table can be found here [[26]](#references)), with final subject category labels being : STEM, Law & Ethics, Social Sciences, Medicine & Health, Humanities, Business & Professional, Global Studies, and Other. The default prompt template used was:

> ```
> Question: What is the capital of France?
> 
> Options:
> A) Paris
> B) London
> C) Berlin
> D) Madrid
> 
> Please respond with ONLY the letter (A-D) corresponding to what you believe is the correct answer.
> ```

This structured format was chosen to promote clear, unambiguous responses from the models. Out of the combined training and test sets, Claude 3.5 Sonnet declined to answer 84 questions, while DeepSeek-v3 declined 43. DeepSeek-v3's refusals consistently included questions concerning Chinese political figures, suggesting potential content filtering. Despite explicit instructions to provide only letter responses, the models differed in their adherence: Claude 3.5 Sonnet reliably provided single-letter answers, while DeepSeek-v3 often included verbose explanations for a particular choice. Accuracy for each model on both the training and test set are summarised in [Figure 1](#Figure1).


<figure id="Figure1">
  <div style="display: flex; flex-direction: column; align-items: center;">
  <img src="/figures_mmlu_eval/baseline_bootstrap_results.png" alt="Baseline MMLU" width="600"/>
  <figcaption>Figure 1: Baseline train and test performance of LLMs on MMLU.</figcaption>
  </div>
</figure>

The evaluation reveals interesting performance dynamics between DeepSeek-v3 and Claude 3.5 Sonnet on the MMLU benchmark. DeepSeek-v3 demonstrates superior performance, achieving 92.0% accuracy on the training set and 83.6% on the test set, significantly outperforming Claude 3.5 Sonnet which scored 84.5% and 69.3% respectively. Both models exhibit a notable train-test generalisation gap - approximately 8.0% for DeepSeek-v3 and 15.0% percentage points for Claude 3.5 Sonnet. The presence of these gaps, along with their narrow confidence intervals, strongly suggests that neither model was exposed to test data during their development, maintaining the integrity of the evaluation. This substantial performance difference between training and testing scenarios provides valuable insights into both models' true capabilities and their ability to generalise to unseen questions. A detailed breakdown of test set performance for each model on a subject level is summarised in [Figure 2](#Figure2).

<figure id="Figure2">
  <div style="display: flex; flex-direction: column; align-items: center;">
    <img src="/figures_mmlu_eval/claude_by_subject.png" alt="Claude MMLU test by subject" width="600"/>
    <img src="/figures_mmlu_eval/deepseek_by_subject.png" alt="Deepseek MMLU test by subject" width="600"/>
    <figcaption>Figure 2: Baseline test set performance of each model by subject.</figcaption>
  </div>
</figure>

The subject-level analysis reinforces DeepSeek-v3's comprehensive superiority across all knowledge domains in the MMLU benchmark. While DeepSeek-v3 consistently outperforms Claude 3.5 Sonnet by large margins, both models actually display remarkably similar patterns in their relative strengths and weaknesses. For instance, both models find Law & Ethics most challenging (70.9% and 56.9% respectively), excel at "Other" category (94.5% and 85.7%) which contains general knowledge, and show strong performance in Social Sciences. This parallel pattern of relative performance across subjects, despite the absolute performance gap, suggests these models may be learning similar underlying representations of domain difficulty, even though DeepSeek-v3 demonstrates more robust capabilities overall. To investigate potential answer biases, I analysed the distribution of answer choices from each model, and compared to the correct ground truth distribution [Figure 3](#Figure3).

<figure id="Figure3">
  <div style="display: flex; flex-direction: column; align-items: center;">
    <img src="/figures_mmlu_eval/baseline_train_distribution.png" alt="Train answer distribution" width="600"/>
    <img src="/figures_mmlu_eval/baseline_test_distribution.png" alt="Test answer distribution" width="600"/>
    <figcaption>Figure 3: Train and test answer distributions for baseline models.</figcaption>
  </div>
</figure>

A closer examination of answer distributions reveals intriguing response patterns, especially for Claude 3.5 Sonnet. While DeepSeek-v3 maintains a relatively uniform distribution across answer choices (A through D) that closely mirrors the ground truth distribution, Claude 3.5 Sonnet exhibits a notable bias in its response pattern. Claude shows a marked reluctance to select option A, choosing it only 17.7% and 11.9% of the time in train and test sets respectively, despite the ground truth distribution showing A should be chosen about 22% of the time. This systematic bias could stem from several factors, including a possible overcorrection for the common human tendency to select 'A' too frequently in multiple-choice scenarios. To better understand this phenomenon, future work could examine Claude's performance across different multiple-choice benchmarks - this would help determine whether the bias is MMLU-specific or a more general characteristic of Claude's decision-making process. Additionally, investigating alternative answer choice formats (such as B-E or i-iv) could provide insights into whether this bias is specifically linked to alphabetical labeling or represents a broader positional preference in Claude's response patterns.


### (ii) Surface level prompt perturbations

To evaluate model robustness to surface-level modifications, I investigated four distinct types of prompt perturbations, each probing different aspects of model behaviour while maintaining semantic equivalence. Examples of these prompt perturbations can be found in my github repo [[27]](#references).

The first perturbation involved permuting multiple-choice answer orderings. While this preserves all semantic information, prior work has shown models can be surprisingly sensitive to answer ordering, suggesting potential memorisation rather than true comprehension. I included this to establish a baseline comparison with previous findings.

My second and third perturbations explored robustness to case transformations: uniform uppercase and random casing. These preserve lexical information while forcing different tokenisation patterns. This is particularly revealing in technical domains where capitalisation carries semantic meaning - like mathematical expressions ('p' vs 'P' in probability) or chemical formulae ('Co' vs 'CO').

The fourth perturbation duplicated incorrect answer choices while maintaining a single instance of the correct answer. I chose to also permute the ordering of choices, as otherwise the format would always has three incorrect answers for E,F, and G. A robust model should actually perform better here since the correct answer becomes uniquely identifiable by frequency alone. Poor performance would suggest the attention layers may be getting overwhelmed by or overly fixating on repeated tokens, rather than leveraging this signal to identify the unique correct answer.

These perturbations form a complementary set: answer permutation tests ordering invariance, case transformations examine tokenisation robustness, and answer duplication probes attention mechanism behaviour with repeated information. Together, they help reveal whether current architectures truly capture semantic content or are overly sensitive to surface-level input features. Results comparing baseline performance to each perturbation is summarised in [Figure 4](#Figure4).

<figure id="Figure4">
  <div style="display: flex; flex-direction: column; align-items: center;">
    <img src="/figures_mmlu_eval/claude_prompt_sensitivity.png" alt="Claude prompt sensitivity 1" width="600"/>
    <img src="/figures_mmlu_eval/deepseek_prompt_sensitivity.png" alt="Deepseek prompt sensitivity 1" width="600"/>
    <figcaption>Figure 4: Superficial prompt sensitivity of both LLMs.</figcaption>
  </div>
</figure>

The results reveal a clear contrast in prompt sensitivity between these models. Claude 3.5 Sonnet shows a statistically significant performance degradation across three of four perturbations:

* Baseline accuracy of 69.3% drops to 66.3% with permuted answers
* Minor variations with case changes (71.2% uppercase, 67.2% random case)
* Most notably, a sharp decline to 59.4% with duplicate wrong answers

In contrast, DeepSeek-v3 demonstrates remarkable robustness:

* Maintains ~83% accuracy across permutation, uppercase, and duplicate wrongs
* Only slight sensitivity to random casing (78.4%)
* Baseline performance (83.6%) virtually unchanged by most perturbations

This suggests DeepSeek-v3 has more robust semantic understanding, while Claude's performance appears more dependent on specific input formats. I also analysed subject level breakdowns for particularly interesting perturbations. DeepSeek-v3's only statistically significant drop in performance was on random casing changes. While I initially hypothesised this might reflect tokenisation issues in jargon-heavy domains, the pattern is less clear, as shown in [Figure 5](#Figure5).

<figure id="Figure5">
  <div style="display: flex; flex-direction: column; align-items: center;">
  <img src="/figures_mmlu_eval/deepseek_random.png" alt="Deepseek random casing per subject" width="600"/>
  <figcaption>Figure 5: DeepSeek-v3 subject level performance with random casing perturbations. (* denote statisically significant differences).</figcaption>
  </div>
</figure>


The largest drop observed was for Claude 3.5 Sonnet when duplicating the wrong answers. The subject-level breakdown in [Figure 6](#Figure6) shows consistent degradation across all domains, suggesting a fundamental architectural limitation rather than domain-specific effects.

<figure id="Figure6">
  <div style="display: flex; flex-direction: column; align-items: center;">
  <img src="/figures_mmlu_eval/claude_duplicate.png" alt="Claude duplicates per subject" width="600"/>
  <figcaption>Figure 6: Claude 3.5 Sonnet subject level performance with incorrect answer duplication. (* denote statisically significant differences).</figcaption>
  </div>
</figure>

I believe there are two plausible explanations for Claude's concerning sensitivity to duplicate wrong answers:
 * Out-of-distribution effects from extending choices from four to seven options.
 * A more general bias towards repeated tokens in a sequence, perhaps due to some attention mechanism effect.

The subject-level breakdown in Figure 6 shows consistent degradation across all domains, suggesting a fundamental architectural or training limitation rather than domain-specific effects. I investigate the first hypothesis in the next section, while the second will be tested in future work by examining if repeating only correct answers reverse the observed trends.


### (iii) LLM generated content sensitivity

For answer generation, I sampled 2,000 MMLU questions across Law & Ethics, STEM, and Social Sciences - a subset chosen to enable statistically significant subgroup analysis. Using identical prompting for both Claude 3.5 Sonnet and DeepSeek-v3, I generated three new incorrect answers per question while preserving the original correct answer. The prompt format was:


> ```
> Given this multiple choice question and its answers, generate 3 new alternative wrong answers. The new wrong answers should be significantly difficult but still clearly incorrect.
> 
> Question: "..."
>
> Correct answer: "..."
>
> Current wrong answers: "...", "...", "..."
> 
> Provide exactly 3 new wrong answers, one per line, no labels or prefixes. Each answer should be relevant to the question domain and challenging but clearly incorrect to a knowledgeable person.
> ```

Of the 2,000 subsampled questions, valid synthetic choices were output on 1,993: 726 on Law & Ethics, 707 on STEM, and 595 on Social Sciences. I looked at text similarity between both LLM responses, as well as each LLM response and the original set of wrong answers. Text similarity was computed as the maximum of word-level Jaccard similarity and sequence-level ratio, to capture both semantic and structural similarities between answers. For each generated wrong answer, I found its highest similarity with any of the three answers in the reference choices, then averaged these maximum similarities for the question in the subsampled set. The analysis of wrong answer generation patterns across both LLMs revealed relatively low semantic overlap between generated and original wrong answers, with mean similarities of 0.52 &plusmn; 0.24 and 0.54 &plusmn; 0.23 respectively for Claude 3.5 Sonnet and DeepSeek-v3. The models showed slightly higher similarity to each other's generated wrong answers 0.61 &plusmn; 0.21, though this level of overlap is still modest. The consistent and substantial standard deviations indicate high variability in the similarity scores. These results suggest that both models generate diverse wrong answers, exploring different parts of the possible answer space rather than converging on similar patterns or reproducing the original wrong answers from the MMLU dataset. The generated wrong answers can be found in my github repo [[28]](#references) for readers interested in examining their quality.
 
To evaluate each model's sensitivity to synthetic generated content, I ran two experimental configurations. The first mimicked the original MMLU formatting, with one correct, and three synthetic incorrect choices. The second included all possibilities, namely one correct, three original incorrect, and three synthetic incorrect choices. In both experiments the order of answer choices was permuted. The reference performance for the four question configuration is the permuted experiment in section (ii), and the reference for the seven question configuration is the duplication of wrongs in section (ii). Results for the four question configuration is summarised in [Figure 7](#Figure7)

<figure id="Figure7">
  <div style="display: flex; flex-direction: column; align-items: center;">
    <img src="/figures_mmlu_eval/claude_on_generateds.png" alt="Claude synthetic four question MMLU" width="600"/>
    <img src="/figures_mmlu_eval/deepseek_on_generateds.png" alt="Deepseek synthetic four question MMLU" width="600"/>
    <figcaption>Figure 7: LLM performance on synthetic MMLU (4 choices per question).</figcaption>
  </div>
</figure>


The results reveal two key patterns in how these models handle synthetic content. First, despite markedly different baseline performances (Claude at 60%, DeepSeek at 80%), both models converge to similar absolute accuracies when evaluated on synthetic content. This convergence may hint at some fundamental property of how language models process carefully crafted incorrect answers. Secondly, and most importantly, the models show opposing behaviours with their own generated content. Claude's peak performance on its own synthetic wrong answers (82.2%) suggests it maintains consistent knowledge - correctly rejecting answers it previously generated as incorrect. DeepSeek's worse performance on its own synthetic answers (71.8%) implies the opposite: it appears biased toward selecting its own generated content, even though it explicitly created these as incorrect answers. The subject level breakdown for each model is illustrated in [Figure 8](#Figure8)



<figure id="Figure8">
  <div style="display: flex; flex-direction: column; align-items: center;">
    <img src="/figures_mmlu_eval/claude_on_generated_per_subject.png" alt="Claude synthetic four question MMLU per subject" width="600"/>
    <img src="/figures_mmlu_eval/deepseek_on_generated_per_subject.png" alt="Deepseek synthetic four question MMLU per subject" width="600"/>
    <figcaption>Figure 8: LLM performance per subject on synthetic MMLU (4 choices per question).</figcaption>
  </div>
</figure>


Breaking down performance by subject area reinforces earlier findings. Across all subjects, Claude consistently improves when evaluated on synthetic incorrect answers, with peak performance on self generated content. DeepSeek shows the opposite pattern across all domains. The consistency of these trends across different subjects suggests these behaviours are fundamental properties of the models rather than domain-specific effects. Finally, the results from the experimental configuration where we utilise all of the original MMLU choices, as well as synthetic generations, is shown in [Figure 9](#Figure9).


<figure id="Figure9">
  <div style="display: flex; flex-direction: column; align-items: center;">
    <img src="/figures_mmlu_eval/claude_on_generateds_incl_origs.png" alt="Claude synthetic four question MMLU per subject" width="600"/>
    <img src="/figures_mmlu_eval/deepseek_on_generateds_incl_origs.png" alt="Deepseek synthetic four question MMLU per subject" width="600"/>
    <figcaption>Figure 9: LLM performance on synthetic MMLU (7 choices per question).</figcaption>
  </div>
</figure>


As a reminder, the reference result in this experiment is where I duplicated all the incorrect answers to maintain seven choices. Claude shows clear improvement when these duplicates are replaced with synthetic content (52.6% to 71.4%), performing best with its own generated wrong answers. This improvement may stem from removing the artificial signal of repeated wrong answers in the prompt - suggesting Claude's higher baseline performance on synthetic content could reflect more genuine reasoning rather than being misled by duplicate options. Additionally, this reinforces our earlier finding that Claude maintains consistent knowledge about what it knows to be incorrect. DeepSeek's performance tells the opposite story: accuracy drops substantially from the reference (81.2%) when evaluated on synthetic content, falling furthest (to 66.3%) when faced with its own generated wrong answers. This shows a concerning bias toward its own generated content, even when that content was explicitly generated as incorrect by the model in a different context. This more complex setup provides stronger evidence for core findings about how these models handle synthetic content.


## Conclusion and future work

This study provides several novel insights into the robustness and reliability of current frontier LLMs. Most notably, it presents what I believe to be the first comprehensive analysis of train-to-test generalisation on the MMLU benchmark. The substantial generalisation gaps observed (8.0% for DeepSeek-v3 and 15.0% for Claude 3.5 Sonnet) provide strong empirical evidence that frontier labs are maintaining proper machine learning practices by not training on test data, despite the commercial pressures and technical ability to do so. This finding validates MMLU's continued utility as a benchmark for assessing general knowledge and reasoning capabilities.

The comparative analyses reveal distinctly different patterns between the two models. DeepSeek-v3 demonstrates superior overall performance, maintaining consistently higher accuracy across both training and test sets, and shows robustness to surface-level prompt perturbations. In contrast, Claude 3.5 Sonnet exhibits significant sensitivity to prompt formatting but demonstrates more consistent reasoning with synthetic content. These divergences suggest that even as models achieve increasingly similar benchmark scores, the growing prevalence of proprietary pre-training techniques, architectural innovations, and post-training alignment strategies is leading to fundamentally different behavioural characteristics. While this study was limited to analysing two frontier models due to computational and time constraints, these stark differences highlight the importance of broader comparative analyses across a larger set of models to better understand the space of possible behaviours and their application beyond benchmark evaluations.

Despite Claude's sensitivity to superficial prompt perturbations, it shows superior reasoning consistency when dealing with LLM-generated content. Its highest performance on prompts containing its own generated incorrect answers suggests it maintains consistent knowledge application across different contexts. In contrast, DeepSeek-v3 shows a potentially concerning bias toward self-generated content. This finding raises serious concerns about the increasing practice of using LLMs to generate content for benchmarks. As already highlighted, MMLU-Pro's approach of using GPT-4-Turbo to expand answer choices from four to seven may inadvertently introduce biases.

These findings have important implications for both model development and evaluation practices. The lack of consistent patterns between just two frontier models suggests the field may still be far from understanding what constitutes optimal or even desirable behavioural characteristics. As we increasingly rely on LLM-generated content for both training and evaluation, understanding these divergences and biases becomes crucial if benchmark evaluations are to continue being the de-facto standard in measuring capabilities.

Future work could explore several promising directions. First, expanding the analysis to a broader set of models would provide more robust insights into behavioural patterns and their relationships to training approaches. This is especially so for models that provide users with token prediction logits or probabilities, allowing for a comprehensive calibration analysis. Secondly (and something I plan to do in the coming weeks when I find the time), investigating whether Claude's sensitivity to repeated tokens extends to correct answers could provide insight into whether this is a consistent trend. Thirdly, developing methods to detect and quantify model biases toward LLM-generated content could help improve benchmark design, ensuring that performance metrics reflect genuine reasoning capabilities. Finally, establishing a standardised protocol for assessing train-test generalisation could provide valuable signals about potential test set contamination in model development.

For my closing thoughts, I find myself in two minds about the implications of these findings. In my own work, I use these models extensively, Claude 3.5 Sonnet assisted in writing much of the codebase for these evaluations, helped sense-check my experimental design and conclusions through lengthy discussions, and collaborated in refining this post. These daily interactions have demonstrated extremely consistent and impressive capabilities that make the models invaluable research tools. However, I also believe the robustness issues identified in this work become particularly critical as we move beyond conversational interfaces toward automated decision-making systems in high-stakes domains. When language models are integrated into healthcare diagnostics, legal document analysis, or financial decision-making systems, they often process poorly structured inputs without human oversight or the iterative refinement possible in research contexts. In these scenarios, sensitivity to prompt formatting or other biases could lead to systematic errors with serious consequences. Understanding these behavioural patterns and limitations is therefore not just an academic exercise, but an important step in ensuring safe and reliable deployment of language models in safety-critical domains. While the findings suggest current frontier models still exhibit concerning brittleness, I also believe making these failure modes more explicit is an important step toward addressing them.

## References

1. AI Safety Fundamentals. <https://aisafetyfundamentals.com/alignment/>

2. MMLU evaluation codebase. <https://github.com/galvsk/mmlu-eval>

3. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt, J. (2021). **Measuring Massive Multitask Language Understanding**. arXiv:2009.03300.

4. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., ... Amodei, D. (2020). **Language Models are Few-Shot Learners.** arXiv preprint arXiv:2005.14165.

5. MMLU-Pro HuggingFace Leaderboard. <https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro>

6. Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). **Building a large annotated corpus of English: The Penn Treebank**. Computational Linguistics, 19(2).

7. Tjong Kim Sang, E. F., & De Meulder, F. (2003). **Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition**. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003.

8. Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002). **BLEU: a method for automatic evaluation of machine translation**. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL).

9. Sutskever, I., Vinyals, O., & Le, Q. V. (2014). **Sequence to sequence learning with neural networks**. Advances in neural information processing systems, 27.

10. Bahdanau, D., Cho, K., & Bengio, Y. (2014). **Neural machine translation by jointly learning to align and translate**. arXiv preprint arXiv:1409.0473.

11. Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. (2019). **SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems**. Advances in Neural Information Processing Systems, 32.

12. Zheng, L., Chiang, W. L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., & Li, H. (2023). **Judging LLM-as-a-judge with MT-Bench and Chatbot Arena**. arXiv preprint arXiv:2306.05685.

13. Oren, Y., Meister, N., Chatterji, N., Ladhak, F., & Hashimoto, T. B. (2023). **Proving test set contamination in black box language models**. arXiv preprint arXiv:2310.17623.

14. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., & Cobbe, K. (2023). **Let's Verify Step by Step**. arXiv:2305.20050.

15. OpenAI o3 ARC-AGI results. <https://arcprize.org/blog/oai-o3-pub-breakthrough>

16. Zheng, C., Zhou, H., Meng, F., Zhou, J., & Huang, M. (2024). **Large Language Models Are Not Robust Multiple Choice Selectors**. arXiv:2309.03882.

17. Sclar, M., Choi, Y., Tsvetkov, Y., & Suhr, A. (2024). **Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting**. arXiv:2310.11324.

18. Alzahrani, N., Alyahya, H. A., Alnumay, Y., Alrashed, S., Alsubaie, S., Almushaykeh, Y., et al. (2024). **When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards**. arXiv:2402.01781.

19. Mirzadeh, I., Alizadeh, K., Shahrokhi, H., Tuzel, O., Bengio, S., & Farajtabar, M. (2024). **GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models**. arXiv:2410.05229.

20. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., et al. (2024). **The Llama 3 Herd of Models**. arXiv:2407.21783.

21. Scheurer, J., Campos, J. A., Chan, J. S., Chen, A., Cho, K., & Perez, E. (2022). **Training Language Models with Language Feedback**. arXiv:2204.14146.

22. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., et al. (2022). **Constitutional AI: Harmlessness from AI Feedback**. arXiv:2212.08073.

23. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., et al. (2024). **MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark**. arXiv:2406.01574.

24. Panickssery, A., Bowman, S. R., & Feng, S. (2024). **LLM Evaluators Recognize and Favor Their Own Generations**. arXiv:2404.13076.

25. DeepSeek-r1 technical report (2025). <https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf>

26. Custom MMLU subject labels. <https://github.com/galvsk/mmlu-eval/blob/main/mmlu_eval/analysis.py#L88>

27. MMLU-Eval Formatter. <https://github.com/galvsk/mmlu-eval/blob/main/mmlu_eval/formatter.py#L178>

28. LLM Generated MMLU Wrong Answers. <https://github.com/galvsk/mmlu-eval/tree/main/data/generated_dataframes>
